{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Với x = 2:\n",
      "ReLU: 2\n",
      "Leaky ReLU: 2\n",
      "Sigmoid: 0.8807970779778823\n",
      "Tanh: 0.964027580075817\n",
      "ELU: 2\n",
      "\n",
      "Với x = -2:\n",
      "ReLU: 0\n",
      "Leaky ReLU: -0.06\n",
      "Sigmoid: 0.1192\n",
      "Tanh: -0.9640\n",
      "ELU: 1.7183\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class ActivationFunction:\n",
    "    def __init__(self, x) -> float:\n",
    "        self.__x = x\n",
    "\n",
    "    def get_x(self):\n",
    "        return self.__x\n",
    "    \n",
    "    def set_x(self, x):\n",
    "        self.__x = x\n",
    "\n",
    "    def ReLU(self):\n",
    "        return max(0, self.__x)\n",
    "    \n",
    "    def Leaky_ReLU(self, alpha = 0.01):\n",
    "        return max(alpha*self.__x, self.__x)\n",
    "    \n",
    "    def Sigmoid(self):\n",
    "        return 1/(1 + math.e **(-self.__x))\n",
    "    \n",
    "    def Tanh(self):\n",
    "        return (math.e**(self.__x) - math.e**(-self.__x))/(math.e**(self.__x) + math.e**(-self.__x))\n",
    "    \n",
    "    def ELU(self, alpha = 1.0):\n",
    "        return max(alpha*(math.e - 1), self.__x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x = 2\n",
    "    activation = ActivationFunction(x)\n",
    "    \n",
    "    print(f\"Với x = {x}:\")\n",
    "    print(f\"ReLU: {activation.ReLU()}\")\n",
    "    print(f\"Leaky ReLU: {activation.Leaky_ReLU(alpha = 0.02)}\")\n",
    "    print(f\"Sigmoid: {activation.Sigmoid()}\")\n",
    "    print(f\"Tanh: {activation.Tanh()}\")\n",
    "    print(f\"ELU: {activation.ELU(alpha = 1.0)}\")\n",
    "    \n",
    "    x = -2\n",
    "    activation.set_x(x)\n",
    "    \n",
    "    print(f\"\\nVới x = {x}:\")\n",
    "    print(f\"ReLU: {activation.ReLU()}\")\n",
    "    print(f\"Leaky ReLU: {activation.Leaky_ReLU(alpha = 0.03)}\")\n",
    "    print(f\"Sigmoid: {activation.Sigmoid():.4f}\")\n",
    "    print(f\"Tanh: {activation.Tanh():.4f}\")\n",
    "    print(f\"ELU: {activation.ELU(alpha = 1.0):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
